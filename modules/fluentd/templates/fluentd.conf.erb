<source>
  @type exec
  command "\
    log stream --color none --level <%= @mac_log_level %> --type log \
      --predicate '(process MATCHES \"(logger|sudo|sshd|firefox|plugin-agent|generic-worker)\") || (messageType > 16)' \
      --timeout 1d \
    | tail -n +3 \
    "
    #  --predicate '(process MATCHES \"(logger|sudo|generic-worker)\") || (messageType > 2)' \
    # messageTypes 0:Default 1:Info 2:Debug ?:Error
    # tail to omit first 2 lines that cause parsing to fail:
    #   Filtering the log data using "type == 1024"
    #   Timestamp                       Thread     Type        Activity             PID    TTL
    #   2019-05-17 22:49:59.070398+0000 0x195      Info        0x0                  0      0    kernel: (IOHIDFamily) HID Activity Tickle (type:0 sender:100007033)
  <parse>
    @type regexp
    expression /^(?<time>.{31}) (?<thread>\S+)\s+(?<severity>\S+)\s+(?<activity>\S+)\s+(?<pid>\S+)\s+(?<ttl>\S+)\s+((?<program>\S+):\s+)?(?<log_message>.*)$/
    types pid:integer
    time_key time
    time_format "%Y-%m-%d %H:%M:%S.%N%z"
    utc true
  </parse>
  tag system
  run_interval 60s  # run command if not still running
</source>

<source>
  @type tail
  path /var/log/genericworker/stderr.log
  pos_file /var/log/genericworker/stderr.log.pos
  <parse>
    @type multiline
    # 2019/11/07 18:41:43 Disk available: 225110376448 bytes\n
    format_firstline /^\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2} /
    format1 /^(?<time>\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2})( [A-Z]{3})? (?<message>.*)/
    time_key time
    time_format "%Y/%m/%d %H:%M:%S"
    utc true
    keep_time_key true
  </parse>
  tag worker.err
</source>
<source>
  @type tail
  path /var/log/genericworker/stdout.log
  pos_file /var/log/genericworker/stdout.log.pos
  <parse>
    @type multiline
    # 2019/11/07 18:41:43 Disk available: 225110376448 bytes\n
    format_firstline /^\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2} /
    format1 /^(?<time>\d{4}\/\d{2}\/\d{2} \d{2}:\d{2}:\d{2})( [A-Z]{3})? (?<message>.*)/
    time_key time
    time_format "%Y/%m/%d %H:%M:%S"
    utc true
    keep_time_key true
  </parse>
  tag worker.info
</source>

<filter worker.**>
  @type record_transformer
  enable_ruby true
  <record>
    program "generic-worker"
<% if @syslog_host != '' -%>
    syslog_severity "${tag_parts[1]}"
    log_message ${record["message"]}
<% end -%>
<% if @stackdriver_clientid != '' -%>
    # stackdriver severities: 
    #   https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity
    stackdriver_severity "${tag_parts[1].gsub(/^(err|info)/i, 'info' => 'info', 'err' => 'error')}"
    message ${record["program"]}: ${record['message']}
<% end -%>
  </record>
</filter>

<filter system>
  @type record_transformer
  enable_ruby true
  <record>
<% if @syslog_host =~ /papertrail/ -%>
    hostname "#{Socket.gethostname}"
<% end -%>
    # Logging pid can be enabled:
    # message pid:${record["pid"]} ${record["message"]}
    # MacOS Unified logging levels: Fault, Error, Default, Info, Debug
<% if @syslog_host != '' -%>
    # syslog severities:
    #   https://github.com/eric/syslog_protocol/blob/master/lib/syslog_protocol/common.rb#L58
    syslog_severity #{record["severity"].gsub(/^(Info|Default|Error|Fault)/i, 'Info' => 'info', 'Default' => 'warn', 'Error' => 'err', 'Fault' => 'crit')}
<% end -%>
<% if @stackdriver_clientid != '' -%>
    message ${record["program"]}: ${record['log_message']}
    # stackdriver severities: 
    #   https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogSeverity
    stackdriver_severity #{record["severity"].gsub(/^(Info|Default|Error|Fault)/i, 'Info' => 'info', 'Default' => 'warning', 'Error' => 'error', 'Fault' => 'critical')}
<% end -%>
  </record>
</filter>

<% if @stackdriver_clientid != '' -%>
# Add a unique insertId to each log entry that doesn't already have it.
# This helps guarantee the order and prevent log duplication.
<filter **>
  @type add_insert_ids
</filter>

# string/numbers only
# msgpack cannot encode ruby Time fields
<filter **>
  @type record_transformer
  remove_keys ["host"]
  <record>
    hostname "#{Socket.gethostname}"
  </record>
  <record>
    workerId "#{Socket.gethostname.split('.')[0]}"
  </record>
  <record>
    workerType "<%= @worker_type %>"
  </record>
  <record>
    workerGroup "#{Socket.gethostname.split('.')[3]}"
  </record>
  <record>
    severity #{record["stackdriver_severity"]}
  </record>
</filter>
<% end -%>

<match fluent.**>
  @type null
</match>

<match **>
  @type copy

<% if @syslog_host =~ /papertrail/ -%>
  <store>
    @type papertrail
    papertrail_host <%= @syslog_host %>
    papertrail_port <%= @syslog_port %>
  </store>
<% elsif @syslog_host != '' -%>
  <store>
    @type remote_syslog
    hostname "#{Socket.gethostname}"
    host <%= @syslog_host %>
    port <%= @syslog_port %>
    <buffer program,syslog_severity>
      flush_at_shutdown true
      overflow_action block
    </buffer>
    program ${program}
    severity ${syslog_severity}
    <format>
      @type single_value
      message_key log_message
    </format>
  </store>
<% else -%>
# Disabled syslog output. (syslog_host: <%= @syslog_host %>)
<% end -%>

<% if @stackdriver_clientid != '' -%>
  <store>
    @type google_cloud
    use_metadata_service false
    vm_id "#{Socket.gethostname}"
    zone "#{Socket.gethostname.split('.')[3]}"
    # Set the chunk limit conservatively to avoid exceeding the recommended
    # chunk size of 5MB per write request.
    buffer_chunk_limit 1M
    # Flush logs every 5 seconds, even if the buffer is not full.
    flush_interval 5s
    # Enforce some limit on the number of retries.
    disable_retry_limit false
    # After 3 retries, a given chunk will be discarded.
    retry_limit 3
    # Wait 10 seconds before the first retry. The wait interval will be doubled on
    # each following retry (20s, 40s...) until it hits the retry limit.
    retry_wait 10
    # Never wait longer than 5 minutes between retries. If the wait interval
    # reaches this limit, the exponentiation stops.
    # Given the default config, this limit should never be reached, but if
    # retry_limit and retry_wait are customized, this limit might take effect.
    max_retry_wait 300
    # Use multiple threads for processing.
    num_threads 8
    detect_json true
    # Use the gRPC transport.
    use_grpc true
    # If a request is a mix of valid log entries and invalid ones, ingest the
    # valid ones and drop the invalid ones instead of dropping everything.
    partial_success true
  </store>
<% else -%>
# Disabled stackdriver output. (clientid: <%= @stackdriver_clientid %>)
<% end -%>
</match>
